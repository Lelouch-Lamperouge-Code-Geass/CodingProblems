# When Array is better than HashMap?


# abstract class vs interface

##### Interface

* interfaces can have no state or implementation  
* a class that implements an interface must provide an implementation of all the methods of that interface  

##### abstrct class

* abstract classes may contain state (data members) and/or implementation (methods)  
* abstract classes can be inherited without implementing the abstract methods (though such a derived class is abstract itself)  
* interfaces may be multiple-inherited, abstract classes may not (this is probably the key concrete reason for interfaces to exist separately from abtract classes - they permit an implementation of multiple inheritance that removes many of the problems of general MI). 


# TCP vs UDP

Transmission Control Protocol is a __connection-oriented protocol__, which means that it requires handshaking to set up end-to-end communications. Once a connection is set up, user data may be sent bi-directionally over the connection.

* Reliable – Strictly only at transport layer, TCP manages message acknowledgment, retransmission and timeout. Multiple attempts to deliver the message are made. If it gets lost along the way, the server will re-request the lost part. In TCP, there's either no missing data, or, in case of multiple timeouts, the connection is dropped. (This reliability however does not cover application layer, at which a separate acknowledgement flow control is still necessary)
* Ordered – If two messages are sent over a connection in sequence, the first message will reach the receiving application first. When data segments arrive in the wrong order, TCP buffers delay the out-of-order data until all data can be properly re-ordered and delivered to the application.
* Heavyweight – TCP requires three packets to set up a socket connection, before any user data can be sent. TCP handles reliability and congestion control.
* Streaming – Data is read as a byte stream, no distinguishing indications are transmitted to signal message (segment) boundaries.
* TCP is a protocol for communication between __exactly two endpoints__. It can't be used for broadcast and multicast.

User Datagram Protocol is a simpler message-based __connectionless protocol__. Connectionless protocols do not set up a dedicated end-to-end connection. Communication is achieved by transmitting information in one direction from source to destination without verifying the readiness or state of the receiver.

* Unreliable – When a UDP message is sent, it cannot be known if it will reach its destination; it could get lost along the way. There is no concept of acknowledgment, retransmission, or timeout.
* Not ordered – If two messages are sent to the same recipient, the order in which they arrive cannot be predicted.
* Lightweight – There is no ordering of messages, no tracking connections, etc. It is a small transport layer designed on top of IP.
* Datagrams – Packets are sent individually and are checked for integrity only if they arrive. Packets have definite boundaries which are honored upon receipt, meaning a read operation at the receiver socket will yield an entire message as it was originally sent.
* No congestion control – UDP itself does not avoid congestion. Congestion control measures must be implemented at the application level.
* Broadcasts - being connectionless, UDP can broadcast - sent packets can be addressed to be receivable by all devices on the subnet.

# socket

Here's a most basic explanation. A socket is defined by an IP address plus a port number. A server listens on a socket for client requests.
 
So, you could write a server that opens a socket connection on your development machine (localhost or IP address 127.0.0.1) on port 80 (default port). If, for example, the server was using the HTTP protocol, then a client could send a request to http://localhost. If the server were listening on port 3000 instead, then the client could send a request to http://localhost:3000.

Sockets are the interface between application layer and transport layers of OSI layer model. It is like a door through which you can go outside and come inside. Similarly, a packet is transferred to application layer from transport layer and vice versa. Every socket has a unique port number. So whenever a client wants to connect to server it must know the port of the socket and ip address. 
Every web application has a socket through which they interact with outer world by sending and receiving packets. The socket residing in server is known as server socket. The socket residing at client side is known as client socket.

 Remember that we need to bind a port number and ip address to server socket but in client socket it is not compulsory. Kernel of the client machine does it for the client.

Let us discuss socket programming. Socket programming is a way of creating sockets for server and for client. There are two major transport layer protocols known as TCP and UDP. Both of these protocols follow different procedure and provide different kind of services so we need to mention specifically inside the socket. There are many system calls and functions through which we can create sockets successfully. I will describe some of them. At server side you require functions like socket(), bind(), listen(), accept(), recvfrom(),sendto() etc.

# Character encoding

Characters that are needed for a specific purpose are grouped into a character set (also called a repertoire). (To refer to characters in an unambiguous way, each character is associated with a number, called a code point.)

The characters are stored in the computer as one or more bytes.

Basically, you can visualise this by assuming that all characters are stored in computers using a special code, like the ciphers used in espionage. A character encoding provides a key to unlock (ie. crack) the code. It is a set of mappings between the bytes in the computer and the characters in the character set. Without the key, the data looks like garbage.

The misleading term charset is often used to refer to what are in reality character encodings. You should be aware of this usage, but stick to using the term character encodings whenever you can.

So, when you input text using a keyboard or in some other way, the character encoding maps characters you choose to specific bytes in computer memory, and then to display the text it reads the bytes back into characters.


In computing, a character encoding is used to represent a repertoire of characters by some kind of encoding system. Depending on the abstraction level and context, corresponding code points and the resulting code space may be regarded as bit patterns, octets, natural numbers, electrical pulses, etc. A character encoding is used in computation, data storage, and transmission of textual data. "Character set", "character map", "codeset" and "code page" are related, but not identical, terms.

Early character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent most of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.

#  float representation

The two most common floating-point binary storage formats used by Intel processors were created for Intel and later standardized by the IEEE organization:

* IEEE Short Real: 32 bits	1 bit for the sign, 8 bits for the exponent, and 23 bits for the mantissa. Also called single precision.
* IEEE Long Real: 64 bits	1 bit for the sign, 11 bits for the exponent, and 52 bits for the mantissa. Also called double precision.


![alt](https://ryanstutorials.net/binary-tutorial/img/floating_point.png)

* https://ryanstutorials.net/binary-tutorial/binary-floating-point.php

# Hash Table

Hash table is a data structure used to implement an associative array, by interpreting the key as an array index so that we can store the value associated with key in array entry, ready for immediate access. On average, its time complexity is constant(amortized).

### Hash collision

Hash table uses hash function to transform the search key into an array index. Ideally, different keys should map to different indices. However, in practical, we generally have to face the possibility that different keys may hash to the same array index. 

##### Collision Resolution

* Separate Chaining : build a linked-list of the key-value pairs whose keys hash to that index. It is called separate chaining because items that collide are chained together in separate linked lists.  
* Open addressing : unlike separate chaining, open addressing tells us that we never use other data structure to store the objects, all entry records are stored in the bucket array itself.  When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some probe sequence, until an unoccupied slot is found.  

##### Probe sequences

Well-known probe sequences include:
* Linear probing, in which the interval between probes is fixed (usually 1)
* Quadratic probing, in which the interval between probes is increased by adding the successive outputs of a quadratic polynomial to the starting value given by the original hash computation
* Double hashing, in which the interval between probes is computed by a second hash function

##### Linear probing
Along with quadratic probing and double hashing, linear probing is a form of open addressing. In these schemes, each cell of a hash table stores a single key–value pair. When the hash function causes a collision by mapping a new key to a cell of the hash table that is already occupied by another key, linear probing searches the table for the closest following free location and inserts the new key there. Lookups are performed in the same way, by searching the table sequentially starting at the position given by the hash function, until finding a cell with a matching key or an empty cell.

Linear probing provides good locality of reference, which causes it to require few uncached memory accesses per operation. Because of this, for low to moderate load factors, it can provide very high performance.d However, compared to some other open addressing strategies, its performance degrades more quickly at high load factors because of primary clustering, a tendency for one collision to cause more nearby collisions. Additionally, achieving good performance with this method requires a higher-quality hash function than for some other collision resolution schemes. When used with low-quality hash functions that fail to eliminate nonuniformities in the input distribution, linear probing can be slower than other open-addressing strategies such as double hashing, which probes a sequence of cells whose separation is determined by a second hash function, or quadratic probing, where the size of each step varies depending on its position within the probe sequence.

##### Deletion

When a key–value pair is deleted, it may be necessary to move another pair backwards into its cell, to prevent searches for the moved key from finding an empty cell.

It is also possible to remove a key–value pair from the dictionary. However, it is not sufficient to do so by simply emptying its cell. This would affect searches for other keys that have a hash value earlier than the emptied cell, but that are stored in a position later than the emptied cell. The emptied cell would cause those searches to incorrectly report that the key is not present.

Instead, when a cell i is emptied, it is necessary to search forward through the following cells of the table until finding either another empty cell or a key that can be moved to cell i v. When an empty cell is found, then emptying cell i is safe and the deletion process terminates. But, when the search finds a key that can be moved to cell i, it performs this move. This has the effect of speeding up later searches for the moved key, but it also empties out another cell, later in the same block of occupied cells. The search for a movable key continues for the new emptied cell, in the same way, until it terminates by reaching a cell that was already empty. In this process of moving keys to earlier cells, each key is examined only once. Therefore, the time to complete the whole process is proportional to the length of the block of occupied cells containing the deleted key, matching the running time of the other hash table operations.

Alternatively, it is possible to use a lazy deletion strategy in which a key–value pair is removed by replacing the value by a special flag value indicating a deleted key. However, these flag values will contribute to the load factor of the hash table. With this strategy, it may become necessary to clean the flag values out of the array and rehash all the remaining key–value pairs once too large a fraction of the array becomes occupied by deleted keys.

# Design Pattern

Software design pattern is a general reusable solution to a commonly occurring problem within a given context in software design. It is a description or template for how to solve a problem that can be used in many different situations. Design patterns are formalized best practices that the programmer can use to solve common problems when designing an application or system.

### Benefits of design pattern

(1) Design patterns can speed up the development process by providing tested, proven development paradigms. Effective software design requires considering issues that may not become visible until later in the implementation. Reusing design patterns helps to prevent subtle issues that can cause major problems and improves code readability for coders and architects familiar with the patterns.

(2) Often, people only understand how to apply certain software design techniques to certain problems. These techniques are difficult to apply to a broader range of problems. Design patterns provide general solutions, documented in a format that doesn't require specifics tied to a particular problem.

(3) In addition, patterns allow developers to communicate using well-known, well understood names for software interactions. Common design patterns can be improved over time, making them more robust than ad-hoc designs.


##### Familiar design patterns:

* Singleton: A class of which only a single instance can exist. Ensure a class only has one instance, and provide a global point of access to it.
* Observer: A way of notifying change to a number of classes. Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically.
* Interpreter: A way to include language elements in a program. Given a language, define a representation for its grammar along with an interpreter that uses the representation to interpret sentences in the language.
* Template: Defer the exact steps of an algorithm to a subclass. Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm’s structure.
* Adapter: Match interfaces of different classes.Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn’t otherwise because of incompatible interfaces.
* Facade: A single class that represents an entire subsystem. Provide a unified interface to a set of interfaces in a system. Facade defines a higher-level interface that makes the subsystem easier to use.
* Factory Method: Creates an instance of several derived classes. Define an interface for creating an object, but let subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses.


# Quick sort vs merge sort

Both are sorting algorithms based on the divide and conquer strategy. Top-down Merge-sort needs an extra array for merge operations, so its space complexity is O(n), while bottom-up merge-sort can have constant space complexity. Merge-sort has O(nlog(n)) time complexity no matter in best/average/worst scenarios. Quick-sort has O(nlog(n)) time complexity in best/average scenario but O(n^2) in worst scenario. Regarding space complexity, the space complexity of quicksort is O(log n), taking into account the stack space used for recursion. Also, quicksort cannot be implemented iteratively, unlike mergesort, where an iterative implementation, sometimes called bottom-up mergesort, is possible. 

# Throughput vs Latency

Latency is the time required to perform some action or to produce some result. Latency is measured in units of time.
Throughput is the number of such actions executed or results produced per unit of time. This is measured in units of whatever is being produced per unit of time.

##### Example:

Latency and throughput can be used together to describe the performance of a network. Latency refers to the time delay between when one machine sends a packet of data and the second machine receives the data (for example, if the second machine receives the data 10 ms later than the first machine sent it, the latency is 10 ms). Throughput refers to the amount of data that can be transferred in a given time (for example, if a one machine sends 1000 KB of data, and it takes 5 seconds for all of it to be received by the second machine, the throughput is 200 KB/s).

Latency – In simplest terms this is Remote Response time. For instance, you want to invoke a web service or access a web page. Apart from the processing time that is needed on the server to process your request, there is a delay involved for your request to reach to server. While referring to Latency, it’s that delay we are talking about. This becomes a big issue for a remote data center which is hosting your service/page. Imagine your data center in US, and accessing it from India. If ignored, latency can trigger your SLA’s. Though it’s quite difficult to improve latency it’s important to measure it. 

Throughput – transactions per second your application can handle (motivation / result of load testing). A typical enterprise application will have lots of users performing lots of different transactions. You should ensure that your application meets the required capacity of enterprise before it hits production. Load testing is the solution for that. Strategy here is to pick up a mix of transactions (frequent, critical, and intensive) and see how many pass successfully in an acceptable time frame governed by your SLAs. 

##### Network Latency

In data network, latency means time when a particular packet takes to reach the destination from source. The term delay is similar to latency. Popular tools like ping and traceroute can be used to measure the delay or latency of the link or connection. Usually this is done by considering time when packet takes to reach to destination from source. The latency or delay can be low if there is high congestion in the traffic or can be because of errors and distance as well.

##### Network Throughput

Network throughput is the amount of data that can traverse through a given medium. The network throughput is measured in bits per second (bps). Throughput can be high or low depending on your network infrastructure. Devices such as routers, switches, firewalls, cables, network cards can have significant impact on the network throughput. High speed devices and cables will definitely increase your network throughput.

# Process vs Thread

Processes are the abstraction of running programs: A binary image, virtualized memory, various kernel resources, an associated security context, and so on. Threads are the unit of execution in a process: A virtualized processor, a stack, and program state. Put another way, processes are running binaries and threads are the smallest unit of execution schedulable by an operating system's process scheduler.

Each process provides the resources needed to execute a program. A process has a virtual address space, executable code, open handles to system objects, a security context, a unique process identifier, environment variables, a priority class, minimum and maximum working set sizes, and at least one thread of execution. Each process is started with a single thread, often called the primary thread, but can create additional threads from any of its threads. 

A thread is the entity within a process that can be scheduled for execution. All threads of a process share its virtual address space and system resources. In addition, each thread maintains exception handlers, a scheduling priority, thread local storage, a unique thread identifier, and a set of structures the system will use to save the thread context until it is scheduled. 

Typical difference is： 

* processes run in separated memory while threads run in shared memory.   
* processes are typically independent, while threads exist as subsets of a process   
* processes carry considerably more state information than threads, whereas multiple threads within a process share process state as well as memory and other resources   
* processes have separate address spaces, whereas threads share their address space processes interact only through system-provided inter-process communication mechanisms   
* context switching between threads in the same process is typically faster than context switching between processes.  

# IPC vs ITC (Inter-Process Communication vs Inter-Thread Communications) 

### IPC:

* File ： A record stored on disk, or a record synthesized on demand by a file server, which can be accessed by multiple processes. 
* Socket  ： A data stream sent over a network interface, either to a different process on the same computer or to another computer on the network. Typically byte-oriented, sockets rarely preserve message boundaries. Data written through a socket requires formatting to preserve message boundaries. 
* Message Queue ：A data stream similar to a socket, but which usually preserves message boundaries. Typically implemented by the operating system, they allow multiple processes to read and write to the message queue without being directly connected to each other. 
* Pipe ： A unidirectional data channel. Data written to the write end of the pipe is buffered by the operating system until it is read from the read end of the pipe. Two-way data streams between processes can be achieved by creating two pipes utilizing standard input and output. Like when we are using arrow symbol in command line. 
* Shared memory ： Multiple processes are given access to the same block of memory which creates a shared buffer for the processes to communicate with each other. 
* Semaphore ： A simple structure that synchronizes multiple processes acting on shared resources. 

### ITC: 

* Synchronization ： primitives, like locks and semaphores 
* Through Events: wait, notify 
* Shared memory, cause typically they all live in the same process Each thread has a private stack, which it can quickly add and remove items from. This makes stack based memory fast, but if you use too much stack memory, as occurs in infinite recursion, you will get a stack overflow. All threads share a common heap. Since all threads share the same heap, access to the allocator/deallocator must be synchronized. There are various methods and libraries for avoiding allocator contention. Some languages allow you to create private pools of memory, or individual heaps, which you can assign to a single thread. every thread would be allocated its own memory space in stack while typically there is only one heap within one process. This means heap space is shared among all threads. Since it is global, it is faster in speed. But also, this causes synchronization issues, which could possibly slow the whole system down. Some languages or OS my support allocating heaps for each thread.


